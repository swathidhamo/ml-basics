{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#sklearn.datasets.make_classification(n_samples=10,n_features=20, n_classes=2)\n",
    "#sklearn.datasets.make_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swathi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:28: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "/home/swathi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:29: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cross_validation import train_test_split\n",
    "mean_1 = []\n",
    "mean_2 = []\n",
    "std = []\n",
    "covariance = []\n",
    "for i in range(0, 20):\n",
    "    mean_1.append(random.uniform(1, 10))\n",
    "    mean_2.append(random.uniform(1, 10))\n",
    "    std.append(random.uniform(1, 10))\n",
    "\n",
    "for i in range(0, 20):\n",
    "    temp = []\n",
    "    for j in range(0, 20):\n",
    "        if i==j:\n",
    "            temp.append(std[i])\n",
    "        else:\n",
    "            k = random.uniform(0.1, 10)\n",
    "            if k < 0:\n",
    "                k = k * -1\n",
    "            temp.append(k)\n",
    "    covariance.append(temp)\n",
    "covariance\n",
    "length = 2000\n",
    "output1 = []\n",
    "output2 = []\n",
    "# a matrix of random values shaped (1000,1000)\n",
    "draw1 = np.random.multivariate_normal(mean_1, covariance,  length)\n",
    "draw2 = np.random.multivariate_normal(mean_2, covariance,  length)\n",
    "#X_train = [], Y_train = [], X_test = [], Y_test = []\n",
    "for i in range(0, length):\n",
    "    output1.append(0);\n",
    "    output2.append(1); \n",
    "X_train_1, X_test_1,Y_train_1, Y_test_1 = train_test_split(draw1, output1, test_size=0.3)\n",
    "X_train_2, X_test_2,Y_train_2, Y_test_2  = train_test_split(draw2, output2, test_size=0.3)\n",
    "\n",
    " \n",
    "X_train = np.concatenate((X_train_1, X_train_2), axis=0)\n",
    "Y_train = np.concatenate((Y_train_1, Y_train_2), axis=0)\n",
    "Y_test = np.concatenate((Y_test_1, Y_test_2), axis=0)\n",
    "X_test = np.concatenate((X_test_1, X_test_2), axis=0)\n",
    "print X_train.shape\n",
    "#plt.scatter(draw[:1,'o',c='green');\n",
    "#plt.scatter(nexte,'o',c='blue')\n",
    "#plt.axis('equal');\n",
    "#print \"First dist\", draw, \"\\n\", \"Second dist\", draw2\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 9.99200722163e-16\n",
      "Average precision: 1.0\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Accuracy: 79.3333%\n",
      "Log Loss: 0.68073192899\n",
      "Average precision: 0.853773584906\n",
      "==============================\n",
      "NuSVC\n",
      "****Results****\n",
      "Accuracy: 99.8333%\n",
      "Log Loss: 0.00778092473358\n",
      "Average precision: 0.998338870432\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "Accuracy: 99.3333%\n",
      "Log Loss: 0.230258509299\n",
      "Average precision: 0.99459717608\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Accuracy: 99.9167%\n",
      "Log Loss: 0.00649944774244\n",
      "Average precision: 0.999168053245\n",
      "==============================\n",
      "AdaBoostClassifier\n",
      "****Results****\n",
      "Accuracy: 99.9167%\n",
      "Log Loss: 0.0578043162612\n",
      "Average precision: 0.999168053245\n",
      "==============================\n",
      "GradientBoostingClassifier\n",
      "****Results****\n",
      "Accuracy: 99.9167%\n",
      "Log Loss: 0.00641391023377\n",
      "Average precision: 0.999583333333\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 5.36051481511e-08\n",
      "Average precision: 1.0\n",
      "==============================\n",
      "LinearDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 9.99200722163e-16\n",
      "Average precision: 1.0\n",
      "==============================\n",
      "QuadraticDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 100.0000%\n",
      "Log Loss: 9.99200722163e-16\n",
      "Average precision: 1.0\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, Y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(Y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    from sklearn.metrics import average_precision_score\n",
    "    Y_score = clf.predict(X_test)\n",
    "    average_precision = average_precision_score(Y_test, Y_score)\n",
    "    print(\"Average precision: {}\".format(average_precision))\n",
    "    \n",
    "    \n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'warm_start': False, 'loss': 'hinge', 'n_jobs': 1, 'eta0': 0.0, 'verbose': 0, 'shuffle': True, 'fit_intercept': True, 'epsilon': 0.1, 'average': False, 'n_iter': 5, 'penalty': 'l2', 'power_t': 0.5, 'random_state': None, 'l1_ratio': 0.15, 'alpha': 0.0001, 'learning_rate': 'optimal', 'class_weight': None}\n",
      "The precision was\n",
      "1.0\n",
      "The recall was\n",
      "0.0\n",
      "Average precision-recall score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "print clf.get_params()\n",
    "Y_score = clf.predict(X_test)\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(Y_test, Y_score)\n",
    "#calculating the precision and the recall values\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "#true positive, false positive and false negatives are computed\n",
    "for i in range(len(Y_test)):\n",
    "    if(Y_test[i]==1 & Y_score[i] == 1):\n",
    "        tp = tp + 1\n",
    "    if(Y_test[i]==1 & Y_score[i] ==0):\n",
    "        fn = fn + 1\n",
    "    if(Y_test[i]==0 & Y_score[i]==1):\n",
    "        fp = fp + 1\n",
    "\n",
    "print 'The precision was'\n",
    "print float(tp/(tp+fp))\n",
    "print 'The recall was'\n",
    "print float(tp/(tp+fn))\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
