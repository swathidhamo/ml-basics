{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix \n",
    "#from sklearn.mixture import BayesianGaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Creating the vocabulary\n",
    "def create_vocabulary(start,end):\n",
    "    vocab = []\n",
    "    overall_length = 0\n",
    "    for i in range(start, end):\n",
    "        path_Name = \"./datasets/Q10/part\"+str(i)\n",
    "        file_count = ([f for f in os.walk(path_Name).next()[2] if f[-4:] == \".txt\"])\n",
    "        overall_length += len(file_count)\n",
    "        for files in file_count:\n",
    "            file_Name = path_Name+'/'+files\n",
    "            my_File = open(file_Name, 'r')\n",
    "        #Read the file\n",
    "            read_File = my_File.read()\n",
    "        #Split the words\n",
    "            words = read_File.split()\n",
    "            words.remove('Subject:')\n",
    "            for word in words:\n",
    "                vocab.append(word)\n",
    "    #Using a set will only save the unique words\n",
    "    #unique_words = set(words)\n",
    "    #You can then print the set as a whole or loop through the set etc\n",
    "    dictionary = Counter(vocab)\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finding the overall length\n",
    "def measure_length(start,end):\n",
    "    \n",
    "    overall_length = 0\n",
    "    for i in range(start, end):\n",
    "        path_Name = \"./datasets/Q10/part\"+str(i)\n",
    "        file_count = ([f for f in os.walk(path_Name).next()[2] if f[-4:] == \".txt\"])\n",
    "        overall_length += len(file_count)  \n",
    "    return overall_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now to create the frequency matrix\n",
    "def create_feature(start, end):\n",
    "    dictionary = create_vocabulary(start, end)\n",
    "    overall_length = measure_length(start, end)\n",
    "    features_matrix = np.zeros((overall_length,len(dictionary)))\n",
    "    train_labels = np.zeros(overall_length)\n",
    "    iterate = 0\n",
    "    for i in range(start, end):\n",
    "        path_Name = \"./datasets/Q10/part\"+str(i)\n",
    "        file_count = ([f for f in os.walk(path_Name).next()[2] if f[-4:] == \".txt\"])\n",
    "        docID = 0;\n",
    "        for files in file_count:\n",
    "            file_Name = path_Name+'/'+files\n",
    "            if(files.find('spmsg')!=-1):\n",
    "                train_labels[iterate] = 1 \n",
    "            my_File = open(file_Name, 'r')\n",
    "        #Read the file\n",
    "            read_File = my_File.read()\n",
    "        #Split the words\n",
    "            words = read_File.split()\n",
    "            words.remove('Subject:')\n",
    "            for word in words:\n",
    "                wordID = 0;\n",
    "                for i,d in enumerate(dictionary):\n",
    "                    if d[0] == word:\n",
    "                    #print \"hi\"     \n",
    "                        wordID = i\n",
    "                        features_matrix[docID,wordID] = words.count(word)\n",
    "            iterate = iterate + 1\n",
    "    return features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ca7f6cd60d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "train_matrix = create_feature(1, 9)\n",
    "model1 = MultinomialNB()\n",
    "model2 = GaussianNB()\n",
    "model3 = BernoulliNB()\n",
    "\n",
    "model1.fit(train_matrix,train_labels)\n",
    "model2.fit(train_matrix,train_labels)\n",
    "model3.fit(train_matrix,train_labels)\n",
    "\n",
    "test_matrix = create_feature(9, 11)\n",
    "\n",
    "result1 = model1.predict(test_matrix)\n",
    "result2 = model2.predict(test_matrix)\n",
    "result3 = model3.predict(test_matrix)\n",
    "\n",
    "print \"Multinomial NB\"\n",
    "print confusion_matrix(train_labels,result1)\n",
    "print model1.score(test_matrix, train_labels)\n",
    "print \"Gaussian NB\"\n",
    "print confusion_matrix(train_labels,result2)\n",
    "print model2.score(test_matrix, train_labels)\n",
    "print \"Bernoulli NB\"\n",
    "print confusion_matrix(train_labels,result3)\n",
    "print model3.score(test_matrix, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BayesianGaussianMixture' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2518c7b9dbc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BayesianGaussianMixture' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#Now to create the frequency matrix\n",
    "features_matrix = np.zeros((overall_length,400))\n",
    "for i in range(1, 9):\n",
    "    path_Name = \"./datasets/Q10/part\"+str(i)\n",
    "    file_count = ([f for f in os.walk(path_Name).next()[2] if f[-4:] == \".txt\"])\n",
    "    docID = 0;\n",
    "    for files in file_count:\n",
    "        file_Name = path_Name+'/'+files\n",
    "        my_File = open(file_Name, 'r')\n",
    "        #Read the file\n",
    "        read_File = my_File.read()\n",
    "        #Split the words\n",
    "        words = read_File.split()\n",
    "        for word in words:\n",
    "            wordID = 0;\n",
    "            for i,d in enumerate(dictionary):\n",
    "                if d == word:\n",
    "                #print i, d     \n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now to create the frequency matrix\n",
    "features_matrix = np.zeros((overall_length,len(dictionary)))\n",
    "train_labels = np.zeros(overall_length)\n",
    "iterate = 0\n",
    "for i in range(1, 9):\n",
    "    path_Name = \"./datasets/Q10/part\"+str(i)\n",
    "    file_count = ([f for f in os.walk(path_Name).next()[2] if f[-4:] == \".txt\"])\n",
    "    docID = 0;\n",
    "    for files in file_count:\n",
    "        file_Name = path_Name+'/'+files\n",
    "        if(files.find('spmsg')!=-1):\n",
    "            train_labels[iterate] = 1  #set train labels to 1 if the message is a spam mail     \n",
    "        my_File = open(file_Name, 'r')\n",
    "        #Read the file\n",
    "        read_File = my_File.read()\n",
    "        #Split the words\n",
    "        words = read_File.split()\n",
    "        words.remove('Subject:')\n",
    "        for word in words:\n",
    "            wordID = 0;\n",
    "            for i,d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                    #print \"hi\"     \n",
    "                    wordID = i\n",
    "                    features_matrix[docID,wordID] = words.count(word)\n",
    "        iterate = iterate + 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878\n"
     ]
    }
   ],
   "source": [
    "print len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix \n",
    "# Create a dictionary of words with its frequency\n",
    "\n",
    "\n",
    "#dictionary = make_Dictionary(train_dir)\n",
    "\n",
    "# Prepare feature vectors per training mail and its labels\n",
    "\n",
    "#train_labels = np.zeros(overall_length)\n",
    "#train_labels[351:701] = 1\n",
    "#train_matrix = extract_features(train_dir)\n",
    "\n",
    "# Training SVM and Naive bayes classifier\n",
    "train_matrix = features_matrix\n",
    "model1 = MultinomialNB()\n",
    "#model2 = LinearSVC()\n",
    "model1.fit(train_matrix,train_labels)\n",
    "#model2.fit(train_matrix,train_labels)\n",
    "\n",
    "# Test the unseen mails for Spam\n",
    "#test_dir = 'test-mails'\n",
    "#test_matrix = extract_features(test_dir)\n",
    "#test_labels = np.zeros(260)\n",
    "#test_labels[130:260] = 1\n",
    "#result1 = model1.predict(test_matrix)\n",
    "#result2 = model2.predict(test_matrix)\n",
    "#print confusion_matrix(test_labels,result1)\n",
    "#print confusion_matrix(test_labels,result2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
